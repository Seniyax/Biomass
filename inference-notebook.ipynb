{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":14526212,"sourceType":"datasetVersion","datasetId":9233109}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom __future__ import annotations\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional\nfrom collections import OrderedDict\nimport gc\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport timm\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T06:09:34.004840Z","iopub.execute_input":"2026-01-19T06:09:34.005083Z","iopub.status.idle":"2026-01-19T06:10:21.553506Z","shell.execute_reply.started":"2026-01-19T06:09:34.005051Z","shell.execute_reply":"2026-01-19T06:10:21.552605Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"@dataclass\nclass InferenceConfig:\n    test_csv  = Path(\"/kaggle/input/csiro-biomass/test.csv\")\n    test_img = Path(\"/kaggle/input/csiro-biomass/test\")\n    model_dir = Path(\"/kaggle/input/5fold-pytorch-model-weights\")\n    model_name = \"vit_huge_plus_patch16_dinov3.lvd1689m\"\n    img_size : int = 518\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    GPU = torch.cuda.get_device_name(0)\n    folds = 5\n    train_target_cols: list[str] = field(default_factory=lambda: [\n        'Dry_Total_g', 'GDM_g', 'Dry_Green_g'\n    ])\n    all_target_cols: list[str] = field(default_factory=lambda: [\n        'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g'\n    ])\n\n    def display_info(self):\n        print(f\"{'='*70}\")\n        print(f\"Inference Configuration\")\n        print(f\"{'='*70}\")\n        print(f\"Device: {self.device}\")\n        print(f\"GPU: {self.GPU}\")\n        print(f\"Backbone: {self.model_name}\")\n        print(f\"Image Size:{self.img_size}\")\n        print(f\"Folds:{self.folds}\")\n        print(\"TTA : 4 views\")\n        print(f\"{'='*70}\\n\")\n\nclass TTAFactory():\n    def __init__(self,img_size:int):\n        self.img_size = img_size\n        self.normal_transform = [\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ToTensorV2()\n        ]\n    def get_TTA(self) -> list[A.Compose]:\n\n        \n        original = A.Compose([\n            A.Resize(self.img_size,self.img_size),\n            *self.normal_transform\n        ])\n\n        hflip = A.Compose([\n            A.HorizontalFlip(p=1.0),\n            A.Resize(self.img_size,self.img_size),\n            *self.normal_transform\n        ])\n\n\n        vflip = A.Compose([\n            \n            A.VerticalFlip(p=1.0),\n            A.Resize(self.img_size,self.img_size),\n            *self.normal_transform\n        ])\n\n        rotate = A.Compose([\n            \n            A.RandomRotate90(p=1.0),\n            A.Resize(self.img_size,self.img_size),\n            *self.normal_transform\n         ])\n\n        return [original,hflip,vflip,rotate]\n\nclass TestDataset(Dataset):\n    def __init__(self,df:pd.Dataframe,transform_on:A.Compose,img_dir:Path):\n        self.df = df\n        self.transform = transform_on\n        self.img_dir = img_dir\n        self.img_paths = df['image_path'].values\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self,idx:int) -> tuple[torch.Tensor,torch.Tensor]:\n        img_pth = self.img_paths[idx]\n        full_pth = self.img_dir/ Path(img_pth).name\n\n        image = cv2.imread(str(full_pth))\n        if image is None:\n            print(f\"Warning: Failed to load image: {full_pth}\")\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        height, width = image.shape[:2]\n        mid_point = width // 2\n        img_left = image[:, :mid_point]\n        img_right = image[:, mid_point:]\n\n        img_left_tensor = self.transform(image=img_left)['image']\n        img_right_tensor = self.transform(image=img_right)['image']\n        \n        return img_left_tensor, img_right_tensor\n\nclass Regressor(nn.Module):\n    def __init__(self,model_name,pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name,pretrained=pretrained,num_classes=0)\n        self.n_features = self.backbone.num_features\n        self.n_combined = self.n_features * 2\n\n        def make_head():\n            return nn.Sequential(\n                nn.Linear(self.n_combined,self.n_combined // 2),\n                nn.GELU(),\n                nn.Dropout(0.1),\n                nn.Linear(self.n_combined // 2,1)\n            )\n\n        self.head_total = make_head()\n        self.head_gdm = make_head()\n        self.head_green = make_head()\n\n    def forward(self,img_left:torch.tensor,img_right:torch.tensor):\n        \n        feat_left = self.backbone(img_left)\n        feat_right = self.backbone(img_right)\n\n        combined = torch.cat([feat_left,feat_right], dim=1)\n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n\n        return out_total,out_gdm,out_green\n\nclass ModelLoader:\n    def __init__(self,config:InferenceConfig):\n        self.config = config\n\n    def load_fold_models(self) -> list[nn.Module]:\n        print(f\"\\nLoading {self.config.folds} trained models...\")\n\n        models = []\n\n        for fold in range(self.config.folds):\n            model_pth = self.config.model_dir / f'best_model_fold{fold}.pth'\n            \n            if not model_pth.exists():\n                raise FileNotFoundError(f\"Model file not found: {model_pth}\")\n\n            base_model = Regressor(self.config.model_name,pretrained=False)\n            state_dict  = torch.load(model_pth, map_location=self.config.device)\n            base_model.load_state_dict(state_dict)\n            base_model.eval()  \n            base_model.to(self.config.device)\n\n            models.append(base_model)\n            print(f\"  âœ“ Fold {fold} model loaded\")\n\n\n        print(f\"âœ“ Successfully loaded {len(models)} models\\n\")\n        return models\n\nclass InferenceEngine:\n    def __init__(self,models: list[nn.Module],config:InferenceConfig):\n        self.models = models\n        self.config = config\n\n\n    def predict_single_view(self,loader:DataLoader) -> dict[str,np.ndarray]:\n        view_preds = {'total': [], 'gdm': [], 'green': []}\n        with torch.no_grad():\n            for img_left, img_right in tqdm(loader, desc=\"  Predicting\", leave=False):\n                img_left = img_left.to(self.config.device)\n                img_right = img_right.to(self.config.device)\n\n                fold_preds = {'total': [], 'gdm': [], 'green': []}\n                \n                for model in self.models:\n                    pred_total, pred_gdm, pred_green = model(img_left, img_right)\n                    fold_preds['total'].append(pred_total.cpu())\n                    fold_preds['gdm'].append(pred_gdm.cpu())\n                    fold_preds['green'].append(pred_green.cpu())\n                    \n                avg_total = torch.mean(torch.stack(fold_preds['total']), dim=0)\n                avg_gdm = torch.mean(torch.stack(fold_preds['gdm']), dim=0)\n                avg_green = torch.mean(torch.stack(fold_preds['green']), dim=0)\n\n                view_preds['total'].append(avg_total.numpy())\n                view_preds['gdm'].append(avg_gdm.numpy())\n                view_preds['green'].append(avg_green.numpy())\n\n        return {\n            k: np.concatenate(v).flatten()\n            for k, v in view_preds.items()\n        }\n\n    def predict_with_tta(self,test_df:pd.DataFrame,tta_transforms:list[A.Compose]) -> dict[str,np.ndarray]:\n        \n         \n        print(f\"\\nStarting TTA inference: {len(tta_transforms)} Views Ã— {self.config.folds} Folds\")\n        all_view_preds: list[dict[str, np.ndarray]] = []\n        for i, transform in enumerate(tta_transforms):\n            \n            print(f\"--- TTA View {i+1}/{len(tta_transforms)} ---\")\n\n            dataset = TestDataset(test_df,transform,self.config.test_img)\n            loader = DataLoader(dataset,batch_size=16,shuffle=False,num_workers = 1)\n\n            view_preds = self.predict_single_view(loader)\n            all_view_preds.append(view_preds)\n\n            print(f\"  âœ“ View {i+1} completed\")\n\n        print(\"\\nCalculating TTA Ensemble (averaging all views)...\")\n        final_preds = {\n            \n            'total': np.mean([p['total'] for p in all_view_preds], axis=0),\n            'gdm': np.mean([p['gdm'] for p in all_view_preds], axis=0),\n            'green': np.mean([p['green'] for p in all_view_preds], axis=0)\n        }\n        \n        print(\"âœ“ Inference completed\\n\")\n        return final_preds\n\nclass submissioncalculator:\n    def __init__(self,config:InferenceConfig):\n        self.config = config\n\n    def create(self,predictions:dict[str, np.ndarray],test_df_long: pd.DataFrame,test_df_unique: pd.DataFrame) -> None:\n        print(\"Creating Submission CSV\")\n        pred_total = predictions[\"total\"]\n        pred_gdm = predictions[\"gdm\"]\n        pred_green = predictions[\"green\"]\n\n        pred_clover = np.maximum(0, pred_gdm - pred_green)\n        pred_dead = np.maximum(0, pred_total - pred_gdm)\n\n        preds_wide = pd.DataFrame({\n            'image_path': test_df_unique['image_path'],\n            'Dry_Green_g': pred_green,\n            'Dry_Dead_g': pred_dead,\n            'Dry_Clover_g': pred_clover,\n            'GDM_g': pred_gdm,\n            'Dry_Total_g': pred_total\n        })\n\n        preds_long = preds_wide.melt(\n            id_vars=['image_path'],\n            value_vars=self.config.all_target_cols,\n            var_name='target_name',\n            value_name='target'\n        )\n        submission = pd.merge(\n            test_df_long[['sample_id', 'image_path', 'target_name']],\n            preds_long,\n            on=['image_path', 'target_name'],\n            how='left'\n        )\n        submission = submission[['sample_id', 'target']]\n        submission.to_csv(\"submission.csv\", index=False)\n        print(\"\\nðŸŽ‰ Submission Saved\")\n        print(\"\\n--- First 5 rows ---\")\n        print(submission.head())\n\n\nclass PredictionPipeline:\n    def __init__(self,config:InferenceConfig):\n        self.config = config\n        self.model_loader = ModelLoader(config)\n        self.tta_factory = TTAFactory(config.img_size)\n        self.sub_creator = submissioncalculator(config)\n\n    def run(self) -> None:\n        print(f\"\\n{'='*70}\")\n        print(f\"ðŸš€ Starting Inference Pipeline\")\n        print(f\"{'='*70}\")\n\n        try:\n            test_df_long, test_df_unique = self.load_test_data()\n            models = self.model_loader.load_fold_models()\n            engine = InferenceEngine(models, self.config)\n            tta_transforms = self.tta_factory.get_TTA()\n            predictions = engine.predict_with_tta(test_df_unique, tta_transforms)\n\n            self.sub_creator.create(\n                predictions,\n                test_df_long,\n                test_df_unique\n            )\n\n            print(\"\\nâœ¨ Inference Pipeline Completed âœ¨\")\n\n        except Exception as e:\n            print(f\"\\nâŒ Error occurred: {e}\")\n            raise\n\n        finally:\n            \n            gc.collect()\n            torch.cuda.empty_cache()\n\n    def load_test_data(self) -> tuple[pd.DataFrame,pd.DataFrame]:\n        print(f\"\\nLoading test data: {self.config.test_csv}\")\n        if not self.config.test_csv.exists():\n            raise FileNotFoundError(f\"test.csv not found: {self.config.test_csv}\")\n                                    \n            \n                                    \n        test_df_long = pd.read_csv(self.config.test_csv)\n        test_df_unique = test_df_long.drop_duplicates(\n            subset=['image_path']\n            ).reset_index(drop=True)\n        print(f\"  Long format: {len(test_df_long)} rows\")\n        print(f\"  Unique images: {len(test_df_unique)} images\\n\")\n        \n        return test_df_long, test_df_unique\n\nif __name__ == '__main__':\n    config = InferenceConfig()\n    config.display_info()\n\n    pipeline = PredictionPipeline(config)\n    pipeline.run()\n    print(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T06:10:30.842261Z","iopub.execute_input":"2026-01-19T06:10:30.842924Z","iopub.status.idle":"2026-01-19T06:11:43.285302Z","shell.execute_reply.started":"2026-01-19T06:10:30.842885Z","shell.execute_reply":"2026-01-19T06:11:43.284374Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nInference Configuration\n======================================================================\nDevice: cuda\nGPU: Tesla P100-PCIE-16GB\nBackbone: vit_large_patch14_dinov2.lvd142m\nImage Size:518\nFolds:5\nTTA : 4 views\n======================================================================\n\n\n======================================================================\nðŸš€ Starting Inference Pipeline\n======================================================================\n\nLoading test data: /kaggle/input/csiro-biomass/test.csv\n  Long format: 5 rows\n  Unique images: 1 images\n\n\nLoading 5 trained models...\n  âœ“ Fold 0 model loaded\n  âœ“ Fold 1 model loaded\n  âœ“ Fold 2 model loaded\n  âœ“ Fold 3 model loaded\n  âœ“ Fold 4 model loaded\nâœ“ Successfully loaded 5 models\n\n\nStarting TTA inference: 4 Views Ã— 5 Folds\n--- TTA View 1/4 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"  âœ“ View 1 completed\n--- TTA View 2/4 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"  âœ“ View 2 completed\n--- TTA View 3/4 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"  âœ“ View 3 completed\n--- TTA View 4/4 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"  âœ“ View 4 completed\n\nCalculating TTA Ensemble (averaging all views)...\nâœ“ Inference completed\n\nCreating Submission CSV\n\nðŸŽ‰ Submission Saved\n\n--- First 5 rows ---\n                    sample_id     target\n0  ID1001187975__Dry_Clover_g   4.094429\n1    ID1001187975__Dry_Dead_g  26.391701\n2   ID1001187975__Dry_Green_g  30.003250\n3   ID1001187975__Dry_Total_g  60.489380\n4         ID1001187975__GDM_g  34.097679\n\nâœ¨ Inference Pipeline Completed âœ¨\n======================================================================\n","output_type":"stream"}],"execution_count":2}]}